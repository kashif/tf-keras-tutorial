{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU training with Estimators,`tf.keras` and`tf.data`\n",
    "\n",
    "The [Estimators](https://www.tensorflow.org/programmers_guide/estimators) API is used for training models in a distributed enviornment such as on nodes with multiple GPUs or on many nodes with GPUs. This is particularly useful when training on huge datasets. But we will however present the API for the tiny Fashion-MNIST dataset as usual in the interest of time.\n",
    "\n",
    "Essentially what we want to remember is that a `tf.keras.Model` can be trained with `tf.estimator` API by converting it to an `tf.estimator.Estimator` object via the `tf.keras.estimator.model_to_estimator` method. Once converted we can apply the machinery that `Estimator` provides to train on different hardware configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#!pip install -q -U tensorflow-gpu\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Fashion-MNIST dataset\n",
    "\n",
    "We will use the [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains thousands of grayscale images of [Zalando](https://www.zalando.de/) fashion articles. Getting the training and test data is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  want to convert the pixel values of these images from a number between 0 and 255 to a number between 0 and 1 and convert the dataset to the `[B, H, W, C]` format where `B` is the number of images, `H` and `W` are the height and width and `C` the number of channels (1 for grayscale) of our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = len(train_images)\n",
    "TEST_SIZE = len(test_images)\n",
    "\n",
    "train_images = np.asarray(train_images, dtype=np.float32) / 255\n",
    "\n",
    "# Convert the train images and add channels\n",
    "train_images = train_images.reshape((TRAINING_SIZE, 28, 28, 1))\n",
    "\n",
    "test_images = np.asarray(test_images, dtype=np.float32) / 255\n",
    "# Convert the train images and add channels\n",
    "test_images = test_images.reshape((TEST_SIZE, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to convert the labels from an integer format (e.g., \"2\" or \"Pullover\"), to a [one hot encoding](https://en.wikipedia.org/wiki/One-hot) (e.g., \"0, 0, 1, 0, 0, 0, 0, 0, 0, 0\"). To do so, we'll use the `tf.keras.utils.to_categorical` [function](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many categories we are predicting from (0-9)\n",
    "LABEL_DIMENSIONS = 10\n",
    "\n",
    "train_labels  = tf.keras.utils.to_categorical(train_labels, LABEL_DIMENSIONS)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, LABEL_DIMENSIONS)\n",
    "\n",
    "# Cast the labels to floats, needed later\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a `tf.keras` model\n",
    "\n",
    "We will create our neural network using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential). Keras is a high-level API to build and train deep learning models and is user friendly, modular and easy to extend. `tf.keras` is TensorFlow's implementation of this API and it supports such things as eager execution, `tf.data` pipelines and Estimators.\n",
    "\n",
    "In terms of the architecture we will ConvNets. On a very high level ConvNets are stacks of Convolutional layers (`Conv2D`) and Pooling layers (`MaxPooling2D`). But most importantly they will take for each training example a 3D tensors of shape (`height`, `width`, `channels`) where for the case of grayscale images `channels=1` and return a 3D tensor. \n",
    "\n",
    "Therefore after the ConvNet part we will need to `Flatten` the tensor and add  `Dense` layers, the last one returning the `LABEL_DIMENSIONS` outputs with the `softmax` activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28,28,1))  # Returns a placeholder tensor\n",
    "x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu)(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(64, activation=tf.nn.relu)(x)\n",
    "predictions = tf.keras.layers.Dense(LABEL_DIMENSIONS, activation=tf.nn.softmax)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also choose a TensorFlow optimizer, rather than using one from  `tf.keras.optimizers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Estimator\n",
    "\n",
    "Now to create an Estimator from the compiled Keras model we call the `model_to_estimator` method. Note that the initial model state of the Keras model is preserved in the created Estimator.\n",
    "\n",
    "So what's so good about Estimators? Well to start off with:\n",
    "\n",
    "* you can run Estimator-based models on a local host or an a distributed multi-GPU environment without changing your model\n",
    "* Estimators simplify sharing implementations between model developers\n",
    "* Estimators build the graph for you\n",
    "\n",
    "\n",
    "So how do we go about training our simple `tf.keras` model to use multi-GPUs? We can use the `tf.contrib.MirroredStrategy` paradigm which does in-graph replication with synchronous training. See this talk on [Distributed TensorFlow training](https://www.youtube.com/watch?v=bRMGoPqsn20) for more information about this strategy. Essentially each worker GPU has a copy of the graph and gets a subset of the data on which it computes the local gradients and then waits for all the workers to finish in a synchronous manner. Then the workers communicate their local gradients to each other via a ring Allreduce operation which is typically optimized to reduce network bandwidth and increase through-put. Once all the gradients have arrived each worker averages them and updates its parameter and the next step begins. This is ideal in situations where you have multiple GPUs on a single node connected via some high-speed interconnect.\n",
    "\n",
    "To use this strategy we first create an estimator from the compiled `tf.keras` model and give it the `MirroredStrategy` configuration via the `RunConfig`. This configuration will use all the GPUs but you can also give a `num_gpus` option to  `MirroredStrategy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using the Keras model provided.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpxestavo_\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpxestavo_', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f46a7d24d30>, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f46a7d24e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.contrib.distribute.MirroredStrategy()\n",
    "config = tf.estimator.RunConfig(train_distribute=strategy)\n",
    "\n",
    "estimator = tf.keras.estimator.model_to_estimator(model, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Estimator input function\n",
    "\n",
    "To pipe data into Estimators we need to define a data importing function which returns a `tf.data` dataset of  `(images, labels)` batches of our data. The function below takes in `numpy` arrays and returns the dataset via an ETL process.\n",
    "\n",
    "Note that in the end we are also calling the `prefetch` method which will buffer the data to the GPUs while they are training so that the next batch is ready and waiting for the GPUs rather than having the GPUs wait for the data at each iteration. The GPU might still not be fully utlized and to improve this we can use fused versions of the transformation operations like `shuffle_and_repeat` instead of two seperate operations, but I have kept it here for simplicity.\n",
    "\n",
    "Again we need a function that simply return a `tf.data.Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(images, labels, epochs, batch_size):\n",
    "    # Convert the inputs to a Dataset. (E)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples. (T)\n",
    "    SHUFFLE_SIZE = 5000\n",
    "    dataset = dataset.shuffle(SHUFFLE_SIZE).repeat(epochs).batch(batch_size)\n",
    "    dataset = dataset.prefetch(2)\n",
    "\n",
    "    # Return the dataset. (L)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Estimator\n",
    "\n",
    "Now the good part! We can call the `train` function on our estimator giving it `input_fn` we defined as well as the number of steps of the optimizer we wish to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\n",
      "INFO:tensorflow:Configured nccl all-reduce.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:batch_all_reduce invoked for batches size = 10 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpxestavo_/keras_model.ckpt\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpxestavo_/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3094852, step = 0\n",
      "INFO:tensorflow:global_step/sec: 123.093\n",
      "INFO:tensorflow:loss = 0.60622776, step = 100 (0.814 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.373\n",
      "INFO:tensorflow:loss = 0.4421891, step = 200 (0.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.782\n",
      "INFO:tensorflow:loss = 0.43948475, step = 300 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.381\n",
      "INFO:tensorflow:loss = 0.3611258, step = 400 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.913\n",
      "INFO:tensorflow:loss = 0.3190798, step = 500 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.094\n",
      "INFO:tensorflow:loss = 0.36249477, step = 600 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.209\n",
      "INFO:tensorflow:loss = 0.2998538, step = 700 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.801\n",
      "INFO:tensorflow:loss = 0.33873618, step = 800 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.128\n",
      "INFO:tensorflow:loss = 0.29917005, step = 900 (0.549 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpxestavo_/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.28289568.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f46a911ce10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "STEPS = 1000\n",
    "\n",
    "estimator.train(input_fn=lambda:input_fn(train_images,\n",
    "                                         train_labels,\n",
    "                                         epochs=EPOCHS,\n",
    "                                         batch_size=BATCH_SIZE//2), \n",
    "                steps=STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Estimator\n",
    "\n",
    "In order to check the performance of our model we can then run the `evaluate` method on our Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-17-09:40:38\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpxestavo_/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-17-09:40:39\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.88310546, global_step = 1000, loss = 0.32520944\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /tmp/tmpxestavo_/model.ckpt-1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.88310546, 'loss': 0.32520944, 'global_step': 1000}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=lambda:input_fn(test_images, \n",
    "                                            test_labels,\n",
    "                                            epochs=1,\n",
    "                                            batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retinal OCT (optical coherence tomography) images example\n",
    "\n",
    "To test the scaling performance on s bigger dataset we can use the [Retinal OCT images](https://www.kaggle.com/paultimothymooney/kermany2018) dataset, on of the many great datasets from [Kaggle](https://www.kaggle.com/datasets). This dataset consists of  cross sections of the retinas of living patients grouped into four categories: NORMAL,CNV,DME and DRUSEN. \n",
    "\n",
    "![](https://i.imgur.com/fSTeZMd.png)\n",
    "\n",
    "The dataset consists of a total of 84,495 X-Ray JPEG images, typically  `512x496`, and can be downloaded via the `kaggle` CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d paultimothymooney/kermany2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once downloaded the training and test set is typically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = os.path.join('OCT2017', 'train', '**/*.jpeg')\n",
    "test_folder = os.path.join('OCT2017', 'test', '**/*.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['CNV', 'DME', 'DRUSEN', 'NORMAL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have our Estimator's input function which takes any file pattern and returns resized images and one hot encoded labels as a `tf.data.Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(file_pattern, labels,\n",
    "             image_size=(224,224),\n",
    "             shuffle=False,\n",
    "             batch_size=64, \n",
    "             num_epochs=None, \n",
    "             buffer_size=4096,\n",
    "             prefetch_buffer_size=4):\n",
    "\n",
    "    table = tf.contrib.lookup.index_table_from_tensor(mapping=tf.constant(labels))\n",
    "    num_classes = len(labels)\n",
    "\n",
    "    def _map_func(filename):\n",
    "        label = tf.string_split([filename], delimiter='/').values[-2]\n",
    "        image = tf.image.decode_jpeg(tf.read_file(filename), channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "        image = tf.image.resize_images(image, size=image_size)\n",
    "        return ({\"input_1\": image}, tf.one_hot(table.lookup(label), num_classes))\n",
    "    \n",
    "    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=shuffle)\n",
    "\n",
    "    if num_epochs is not None and shuffle:\n",
    "        dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(buffer_size, num_epochs))\n",
    "    elif shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size)\n",
    "    elif num_epochs is not None:\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "\n",
    "    dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=_map_func,\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          num_parallel_calls=os.cpu_count()))\n",
    "    dataset = dataset.prefetch(buffer_size=prefetch_buffer_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train this we will use a pretrained VGG16 and train just the last 5 layers of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_vgg16 = tf.keras.applications.VGG16(input_shape=(224,224,3),\n",
    "                                          include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = keras_vgg16.output\n",
    "output = tf.keras.layers.Flatten()(output)\n",
    "predictions = tf.keras.layers.Dense(len(labels), activation=tf.nn.softmax)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=keras_vgg16.input, outputs=predictions)\n",
    "\n",
    "for layer in keras_vgg16.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all we need and can proceed as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.contrib.distribute.MirroredStrategy()\n",
    "config = tf.estimator.RunConfig(train_distribute=strategy)\n",
    "estimator = tf.keras.estimator.model_to_estimator(model, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=lambda:input_fn(train_folder,\n",
    "                                         labels,\n",
    "                                         shuffle=True,\n",
    "                                         batch_size=32,\n",
    "                                         buffer_size=2048,\n",
    "                                         num_epochs=20),\n",
    "                steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trainiend we can evaluate the accuracy on the test set (should be around 95%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.evaluate(input_fn=lambda:input_fn(test_folder,\n",
    "                                            labels, \n",
    "                                            shuffle=True,\n",
    "                                            batch_size=64,\n",
    "                                            buffer_size=1024,\n",
    "                                            num_epochs=1,\n",
    "                                            prefetch_buffer_size=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced CNNs with tf.keras and tf.data.\n",
    "\n",
    "\n",
    "### Functional API\n",
    "\n",
    "We saw in the last notebook how to use `tf.keras.Sequential` to stack layers together for the classification task. One issue with the stacking API is that we cannot create arbitrary models topologies, which are the bread and butter of Deep Learning research.  \n",
    "\n",
    "Keras provides a [functional](https://keras.io/getting-started/functional-api-guide/) style of API to build complex model topologies such as:\n",
    "* multi-input models (think images and their descriptions)\n",
    "* multi-output models (think classification and a summary of an image)\n",
    "* models with non-sequential data flows (e.g. skip connections or by-passing parts of the network)\n",
    "\n",
    "So lets rewrite the previous classifier in this API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = len(train_images)\n",
    "TEST_SIZE = len(test_images)\n",
    "\n",
    "train_images = np.asarray(train_images, dtype=np.float32) / 255\n",
    "\n",
    "# Convert the train images and add channels\n",
    "train_images = train_images.reshape((TRAINING_SIZE, 28, 28, 1))\n",
    "\n",
    "test_images = np.asarray(test_images, dtype=np.float32) / 255\n",
    "# Convert the train images and add channels\n",
    "test_images = test_images.reshape((TEST_SIZE, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many categories we are predicting from (0-9)\n",
    "LABEL_DIMENSIONS = 10\n",
    "\n",
    "train_labels  = tf.keras.utils.to_categorical(train_labels, LABEL_DIMENSIONS)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, LABEL_DIMENSIONS)\n",
    "\n",
    "# Cast the labels to floats, needed later\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the input tensor and use the simple rule that **any layer instance is callable on a tensor and will return a tensor**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28,28,1))  # Returns a placeholder tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu)(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(64, activation=tf.nn.relu)(x)\n",
    "predictions = tf.keras.layers.Dense(LABEL_DIMENSIONS, activation=tf.nn.softmax)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model given inputs and outputs.\n",
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model is the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "\n",
    "# Because tf.data may work with potentially **large** collections of data\n",
    "# we do not shuffle the entire dataset by default\n",
    "# Instead, we maintain a buffer of SHUFFLE_SIZE elements\n",
    "# and sample from there.\n",
    "SHUFFLE_SIZE = 10000 \n",
    "\n",
    "# Create the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "dataset = dataset.shuffle(SHUFFLE_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.140625\n",
      "10 0.515625\n",
      "20 0.6484375\n",
      "30 0.6484375\n",
      "40 0.703125\n",
      "50 0.6171875\n",
      "60 0.6640625\n",
      "70 0.625\n",
      "80 0.796875\n",
      "90 0.671875\n",
      "100 0.75\n",
      "110 0.7734375\n",
      "120 0.796875\n",
      "130 0.7890625\n",
      "140 0.7578125\n",
      "150 0.7109375\n",
      "160 0.8203125\n",
      "170 0.7890625\n",
      "180 0.734375\n",
      "190 0.828125\n",
      "200 0.796875\n",
      "210 0.7578125\n",
      "220 0.7421875\n",
      "230 0.796875\n",
      "240 0.8125\n",
      "250 0.7421875\n",
      "260 0.8515625\n",
      "270 0.8046875\n",
      "280 0.859375\n",
      "290 0.90625\n",
      "300 0.8046875\n",
      "310 0.796875\n",
      "320 0.8515625\n",
      "330 0.8359375\n",
      "340 0.84375\n",
      "350 0.8125\n",
      "360 0.765625\n",
      "370 0.890625\n",
      "380 0.8046875\n",
      "390 0.78125\n",
      "400 0.8671875\n",
      "410 0.859375\n",
      "420 0.8359375\n",
      "430 0.8203125\n",
      "440 0.875\n",
      "450 0.7890625\n",
      "460 0.8984375\n",
      "Epoch #1\t Loss: 0.461277\tAccuracy: 0.833333\n",
      "0 0.796875\n",
      "10 0.8984375\n",
      "20 0.8125\n",
      "30 0.8515625\n",
      "40 0.859375\n",
      "50 0.8046875\n",
      "60 0.8359375\n",
      "70 0.8125\n",
      "80 0.921875\n",
      "90 0.796875\n",
      "100 0.8828125\n",
      "110 0.8359375\n",
      "120 0.890625\n",
      "130 0.890625\n",
      "140 0.9140625\n",
      "150 0.890625\n",
      "160 0.90625\n",
      "170 0.859375\n",
      "180 0.7890625\n",
      "190 0.890625\n",
      "200 0.8671875\n",
      "210 0.875\n",
      "220 0.8671875\n",
      "230 0.890625\n",
      "240 0.8359375\n",
      "250 0.875\n",
      "260 0.8984375\n",
      "270 0.890625\n",
      "280 0.8828125\n",
      "290 0.921875\n",
      "300 0.84375\n",
      "310 0.890625\n",
      "320 0.890625\n",
      "330 0.875\n",
      "340 0.875\n",
      "350 0.9140625\n",
      "360 0.8515625\n",
      "370 0.90625\n",
      "380 0.828125\n",
      "390 0.84375\n",
      "400 0.875\n",
      "410 0.90625\n",
      "420 0.8984375\n",
      "430 0.84375\n",
      "440 0.8828125\n",
      "450 0.875\n",
      "460 0.9453125\n",
      "Epoch #2\t Loss: 0.357041\tAccuracy: 0.895833\n",
      "0 0.8203125\n",
      "10 0.9296875\n",
      "20 0.859375\n",
      "30 0.875\n",
      "40 0.8828125\n",
      "50 0.8515625\n",
      "60 0.8359375\n",
      "70 0.859375\n",
      "80 0.921875\n",
      "90 0.828125\n",
      "100 0.90625\n",
      "110 0.8671875\n",
      "120 0.9296875\n",
      "130 0.890625\n",
      "140 0.921875\n",
      "150 0.9140625\n",
      "160 0.921875\n",
      "170 0.8984375\n",
      "180 0.828125\n",
      "190 0.9296875\n",
      "200 0.890625\n",
      "210 0.90625\n",
      "220 0.875\n",
      "230 0.8984375\n",
      "240 0.8515625\n",
      "250 0.859375\n",
      "260 0.9296875\n",
      "270 0.859375\n",
      "280 0.9375\n",
      "290 0.8984375\n",
      "300 0.84375\n",
      "310 0.875\n",
      "320 0.921875\n",
      "330 0.890625\n",
      "340 0.890625\n",
      "350 0.90625\n",
      "360 0.8828125\n",
      "370 0.9453125\n",
      "380 0.8828125\n",
      "390 0.890625\n",
      "400 0.8828125\n",
      "410 0.921875\n",
      "420 0.90625\n",
      "430 0.875\n",
      "440 0.90625\n",
      "450 0.890625\n",
      "460 0.9609375\n",
      "Epoch #3\t Loss: 0.279723\tAccuracy: 0.916667\n",
      "0 0.859375\n",
      "10 0.953125\n",
      "20 0.8984375\n",
      "30 0.890625\n",
      "40 0.890625\n",
      "50 0.8828125\n",
      "60 0.890625\n",
      "70 0.8828125\n",
      "80 0.921875\n",
      "90 0.8671875\n",
      "100 0.9140625\n",
      "110 0.90625\n",
      "120 0.9375\n",
      "130 0.921875\n",
      "140 0.90625\n",
      "150 0.9140625\n",
      "160 0.9375\n",
      "170 0.90625\n",
      "180 0.8515625\n",
      "190 0.9375\n",
      "200 0.921875\n",
      "210 0.90625\n",
      "220 0.8984375\n",
      "230 0.8828125\n",
      "240 0.8671875\n",
      "250 0.921875\n",
      "260 0.9453125\n",
      "270 0.8984375\n",
      "280 0.9453125\n",
      "290 0.9140625\n",
      "300 0.890625\n",
      "310 0.90625\n",
      "320 0.921875\n",
      "330 0.90625\n",
      "340 0.90625\n",
      "350 0.921875\n",
      "360 0.921875\n",
      "370 0.9453125\n",
      "380 0.8984375\n",
      "390 0.9140625\n",
      "400 0.90625\n",
      "410 0.9375\n",
      "420 0.890625\n",
      "430 0.8984375\n",
      "440 0.8984375\n",
      "450 0.90625\n",
      "460 0.953125\n",
      "Epoch #4\t Loss: 0.208543\tAccuracy: 0.927083\n",
      "0 0.8828125\n",
      "10 0.9609375\n",
      "20 0.9296875\n",
      "30 0.921875\n",
      "40 0.8828125\n",
      "50 0.8828125\n",
      "60 0.8984375\n",
      "70 0.8828125\n",
      "80 0.9296875\n",
      "90 0.8984375\n",
      "100 0.921875\n",
      "110 0.9140625\n",
      "120 0.9453125\n",
      "130 0.921875\n",
      "140 0.9140625\n",
      "150 0.9140625\n",
      "160 0.9609375\n",
      "170 0.921875\n",
      "180 0.859375\n",
      "190 0.9375\n",
      "200 0.9296875\n",
      "210 0.921875\n",
      "220 0.921875\n",
      "230 0.890625\n",
      "240 0.90625\n",
      "250 0.9375\n",
      "260 0.9375\n",
      "270 0.8984375\n",
      "280 0.9375\n",
      "290 0.921875\n",
      "300 0.921875\n",
      "310 0.90625\n",
      "320 0.921875\n",
      "330 0.9140625\n",
      "340 0.9140625\n",
      "350 0.9375\n",
      "360 0.9375\n",
      "370 0.9453125\n",
      "380 0.9140625\n",
      "390 0.90625\n",
      "400 0.9140625\n",
      "410 0.921875\n",
      "420 0.921875\n",
      "430 0.90625\n",
      "440 0.9375\n",
      "450 0.9140625\n",
      "460 0.953125\n",
      "Epoch #5\t Loss: 0.168311\tAccuracy: 0.937500\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=5 # or the number of times we go through our entire training dataset\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for (batch, (images, labels)) in enumerate(dataset):\n",
    "        train_loss, train_accuracy = model.train_on_batch(images, labels)\n",
    "    \n",
    "        if batch % 10 == 0: print(batch, train_accuracy)\n",
    "  \n",
    "    # Here you can gather any metrics or adjust your training parameters\n",
    "    print('Epoch #%d\\t Loss: %.6f\\tAccuracy: %.6f' % (epoch + 1, train_loss, train_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again to evaluate the model we need to check the accuracy on unseen or test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000==============================] - 1s 62us/sample - loss: 0.2944 - acc: 0.8920\n",
      "\n",
      "Test Model \t\t Loss: 0.294421\tAccuracy: 0.892000\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('\\nTest Model \\t\\t Loss: %.6f\\tAccuracy: %.6f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Subclassing\n",
    "\n",
    "It is also possible to build a  fully-customizable model by subclassing `tf.keras.Model` and defining your own forward pass (just don't tell that to the PyTorch people ðŸ™ˆ). Just like in PyTorch you create layers in the `__init__` method and set them as attributes of the class instance. Define the forward pass in the `call` method and boom! you are ready to go!\n",
    "\n",
    "This is particularly useful when eager execution is enabled since the forward pass can be written imperatively.\n",
    "\n",
    "```python\n",
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, num_classes=10):\n",
    "    super(MyModel, self).__init__(name='my_model')\n",
    "    self.num_classes = num_classes\n",
    "    # Define your layers here.\n",
    "    self.dense_1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "    self.dense_2 = tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Define your forward pass here,\n",
    "    # using layers you previously defined (in `__init__`).\n",
    "    x = self.dense_1(inputs)\n",
    "    return self.dense_2(x)\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    # You need to override this function if you want to use the subclassed model\n",
    "    # as part of a functional-style model.\n",
    "    # Otherwise, this method is optional.\n",
    "    shape = tf.TensorShape(input_shape).as_list()\n",
    "    shape[-1] = self.num_classes\n",
    "    return tf.TensorShape(shape)\n",
    "\n",
    "\n",
    "# Instantiates the subclassed model.\n",
    "model = MyModel(num_classes=10)\n",
    "```\n",
    "\n",
    "As an exercise write the above model for Fashion-MNIST by subclassing `tf.keras.Model` and train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom layers\n",
    "\n",
    "A lot of times researchers will write their own custom layer which is possible now by subclassing `tf.keras.layers.Layer` and implementing the following methods:\n",
    "\n",
    "* `build`: Create the weights of the layer. Add weights with the `add_weight` method.\n",
    "* `call`: Define the forward pass.\n",
    "* `compute_output_shape`: Specify how to compute the output shape of the layer given the input shape.\n",
    "* Optionally, a layer can be serialized by implementing the `get_config` method and the `from_config` class method.\n",
    "\n",
    "As an example:\n",
    "\n",
    "```python\n",
    "class MyLayer(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, output_dim, **kwargs):\n",
    "    self.output_dim = output_dim\n",
    "    super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    shape = tf.TensorShape((input_shape[1], self.output_dim))\n",
    "    # Create a trainable weight variable for this layer.\n",
    "    self.kernel = self.add_weight(name='kernel',\n",
    "                                  shape=shape,\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "    # Be sure to call this at the end\n",
    "    super(MyLayer, self).build(input_shape)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.matmul(inputs, self.kernel)\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    shape = tf.TensorShape(input_shape).as_list()\n",
    "    shape[-1] = self.output_dim\n",
    "    return tf.TensorShape(shape)\n",
    "\n",
    "  def get_config(self):\n",
    "    base_config = super(MyLayer, self).get_config()\n",
    "    base_config['output_dim'] = self.output_dim\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    return cls(**config)\n",
    "\n",
    "\n",
    "# Create a model using the custom layer\n",
    "model = tf.keras.Sequential([MyLayer(10),\n",
    "                             tf.keras.layers.Activation('softmax')])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
